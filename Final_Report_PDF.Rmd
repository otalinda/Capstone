---
title: 'HarvardX Data Science Final Project: A Classification Task Using Machine Learning
  Techniques'
author: "Gretta Digbeu"
date: "July 28th, 2019"
output: pdf_document
---

```{r, echo=FALSE, message=FALSE, error=FALSE, warning=FALSE,results = "hide"}

library(plyr)
library(tidyverse)
library(caret)
library(randomForest)
library(ggplot2)
library(viridis)
library(ggthemes)
library(ggrepel)
library(stringr)
library(reshape)
library(rockchalk)
library(RColorBrewer)
library(kableExtra)
library(tinytex)
library(reticulate)
library(scales)
library(gridExtra)
library(cowplot)
library(gtable)
library(tinytex)
library(knitr)
library(tibble)
library(xgboost)



options(tinytex.verbose = TRUE)

```

## __A.Introduction__

This machine learning exercise is the final component in the capstone course of the Harvard edX Data Science online professional certificate program. Students pursuing the verified track are asked to apply predictive machine learning techniques using a publicly available dataset to solve a problem of their choice. 

We chose to explore the 1996 _Adult_ dataset, also known as the _Census Income_ dataset, available on the  UCI's Machine Learning Repository. The data was extracted by Silicon Valley researchers (Ronny Kohavi and Barry Becker) from the 1994 Current Population Survey (CPS) data held in the Census database. It is already partitioned into training and testing sets containing 32,561 and 16,281 observations respectively (30,162 and 15,060 after removing unknowns), and includes 14 attributes. The variable of interest, gross income, was discretized into two ranges with a threshold of $50,000. The data is therefore ideal for a multivariate classification exercise with the goal of predicting this binary outcome variable.  

Before exploring the data for trends and patterns, let us present a breakdown of the attributes contained therein:

```{r, echo=FALSE}

attributes <- data.frame(
    attribute = c("age", "workclass", "final_weight", "education_level", "education_num", "marital_status", "occupation", "relationship", "race", "sex", "capital_gain", "capital_loss","hours_per_week","native_country", "income_category"),
    type = c("continuous", "categorical", "continuous", "categorical", "continuous", "categorical", "categorical", "categorical", "categorical", "binary", "continuous", "continuous", "continuous", "categorical", "categorical"),
    description = c("Age in years", "Class of worker", "Demographic weight", "Highest level of education achieved", "Total years of education", "Marital status", "Type of occupation", "Relationship of survey responder to the head of household", "Ethnic origin", "Biological sex", "Profits made from the sale of real estate, investments and personal property", "Losses incurred when capital assets decrease in value", "Average number of hours worked per week", "Country of origin", "Income in relation to 50k"))
```

```{r, echo = FALSE}
attributes %>%
  kable() %>%
  kable_styling(latex_options = c("striped", "hold_position"))
```

We note that there are 14 attributes, 6 of which are continuous and 8 are categorical. This includes the _weight_ feature assigned to each observation by the researchers at the Population Division of the Census Bureau. These weights are controlled to independent estimates of the civilian non-institutional population of the US, using 3 sets of controls: a single cell estimate of the population 16 and up for each state; controls for Hispanic Origin by age and sex; and controls by Race, age and sex. People with similar demographic characteristics should therefore have similar weights, with one important caveat: because the CPS sample is a collection of 51 separate state samples, each with its own probability of selection, such similarities only apply to observations from the same state.

```{r, echo=FALSE, message=FALSE, error=FALSE, warning=FALSE,results = "hide"}

#1)Import
raw_training <- read.table("/Users/grettadigbeu/Desktop/R Files/adult.data", header=FALSE, sep=",", na.strings = "?", strip.white=TRUE)

#2) Assign column names
colnames(raw_training) <- c("age", 
                            "workclass", 
                            "final_weight", 
                            "education_level", 
                            "education_num", 
                            "marital_status", 
                            "occupation", 
                            "relationship", 
                            "race", 
                            "sex", "capital_gain", 
                            "capital_loss", 
                            "hours_per_week", 
                            "native_country", 
                            "income_category")

#3) Remove NAs

training <-na.omit(raw_training)

head(training)
str(training)

#VALIDATION SET#
################

#1)Import
raw_test <- read.table("/Users/grettadigbeu/Desktop/R Files/adult.test", skip = 1, header=FALSE, sep=",", na.strings = "?", strip.white=TRUE)
head(raw_test)

#2)Assign column names
colnames(raw_test) <- c("age", 
                       "workclass", 
                       "final_weight", 
                       "education_level", 
                       "education_num", 
                       "marital_status", 
                       "occupation", 
                       "relationship", 
                       "race", 
                       "sex", 
                       "capital_gain", 
                       "capital_loss", 
                       "hours_per_week", 
                       "native_country", 
                       "income_category")

#3) Remove NAs
testing <- na.omit(raw_test)
str(testing)

length(is.na(testing))

options(tinytex.verbose = TRUE)
```


## __B.Exploratory Data Analysis__

In order to identify trends and patterns in the census data, we must conduct some exploratory data analysis. Our findings will guide the methods employed in this exercise, and help us determine whether any transformations must be made to the data. These include but are not limited to: removing attributes highly correlated with other features, applying log transformations or scaling/standardizing the values of continuous attributes, and removing attributes with very few unique values or close to zero variation in our outcome variable. 

### __I.Data at a Glance__ 

After importing the training and testing sets separately and removing the unknowns, we assign the column names listed above. Note that for ease of understanding, these variable names differ slightly from the column names listed on the UCI Machine Learning Repository. Next, we examine the structure of each subset to ensure that they are structurally identical because we want to explore these trends over the entire sample, as opposed to examining the training and testing sets separately. The results are as follows:

#### __Training Set__
```{r, echo = FALSE}
str(training)
```

#### __Testing Set__
```{r, echo = FALSE}
str(testing)
```

Looking at the structure of each subset reveals that the levels of our variable of interest (income_category), are syntactically different. The levels in the testing set have an unnecessary period at the end of each expression. 


We therefore recode the levels in the testing data so they match the training set:

```{r}
levels(testing$income_category)[1] <- "<=50K"
levels(testing$income_category)[2] <- ">50K"
```


Moreover, a quick look at the relationship between the __education_level__ and __education_num__ variables reveals that the latter does not correspond to the total number of years of education. Rather, it is simply a numeric representation of the __education_level__ attribute:

```{r, echo = FALSE}
t1 <- training %>% distinct(education_level, education_num) %>% arrange(education_num)
t1 <- as.data.frame(t1)

t1 %>% kable() %>% kable_styling(latex_options = c("striped", "hold_position"), full_width = T)
```

We can therefore remove the __education_num__ feature, as it is a redundant variable that provides no additional information about our population.  We remove it from both the testing and training data frames.

```{r, echo = FALSE}
training <- training %>% select(-education_num)
testing <- testing %>% select(-education_num)
```

We can now proceed to bind the rows and explore the full dataset. Excluding the unknowns, our census sample has __45,222__ observations. 

```{r, echo=FALSE}
data_all <- rbind(training, testing)
str(data_all)
```

First, we look at the prevalence of each class of our outcome variable. What is relative proportion of people making more than $50,000 a year in our sample?

```{r, echo=FALSE}
share <- data_all %>%
  group_by(income_category) %>%
  tally() %>%
  mutate(pct = n / sum(n), percentage = paste0(round(pct*100, 1), "%"))
```


```{r, echo=FALSE}
share %>%
  kable() %>%
  kable_styling(latex_options = c("striped", "hold_position"), full_width = T)
```


We note that at 75.4%, the prevalence of people with incomes below or equal to $50,000/year is much higher than that of people making more than $50,000/year. The implication of this uneven prevalence is that when it comes time to assessing an algorithm, instead of taking the balanced accuracy (F1 score) generated from the confusion matrix at face value, we should compute a weighted score using the f_meas() function, adjusting the _Beta_ accordingly. 

### __II.Deep Dive__ 

#### __II.1 Categorical Attributes__

We now proceed to take a deeper dive into a selection of features one by one, starting with the categorical variables. These are __workclass, education_level, marital_status, occupation, relationship, race, sex, and native_country__. These are all factor variables so we take a look at the levels contained in each. 

We notice that the variable __work_class__ (class of worker) has an unused level ("never-worked"), so we drop it from the data.  

```{r, echo = FALSE}
summary(data_all$workclass)

#Remove unused level "never_worked" 
  data_all$workclass <- droplevels(data_all$workclass)

```

Upon summarizing the data along this feature, we note that most survey respondents (73.7%) are private sector workers, and that the highest concentration of individuals earning more than $50K/year is found among the incorporated self-employed workers (self-emp-inc).


```{r, echo=FALSE} 
  #Make the bar graph of frequencies 
  proportion0 <- data_all %>%
    group_by(workclass) %>%
    tally() %>%
    mutate(pct = n / sum(n), y_label = paste0(round(pct*100, 1), "%"))
  

  p0 <- proportion0 %>% 
    ggplot(aes(x = reorder(workclass, -pct), y = pct)) +
    geom_bar(stat = "identity", position = "dodge", fill = "lightgoldenrod1", color = "black", alpha=.9) +
    geom_text(aes(label = y_label), position = position_dodge(width=.9), vjust = -.25) +
    scale_y_continuous(labels = percent_format(accuracy = 1))+
    xlab("")+
    ylab("")+
    theme_clean()+
    ggtitle("Types of Employment") 
    

  #Make stacked bars
  proportion1 <- data_all %>%
    group_by(workclass, income_category) %>%
    tally() %>%
    group_by(workclass) %>%
    mutate(pct = n / sum(n), y_label = paste0(round(pct*100, 1), "%"))
  
  
  p1<- proportion1 %>% 
    ggplot(aes(y = pct, x = workclass, fill = income_category))+
    geom_bar(position = "fill", stat = "identity", alpha = .8) +
    scale_fill_viridis(discrete=TRUE, option = "viridis")+
    geom_text(aes(label = y_label), position = position_stack(vjust = .5), color = "black") +
    xlab("")+
    ylab("")+
    scale_y_continuous(labels = percent_format(accuracy = 1))+
    ggtitle("Breakdown of Income Class by Class of Worker") + 
    theme_clean()
    
```

```{r fig.width=10, fig.height=8, fig.align="left", echo = FALSE}
grid.arrange(p0,p1, nrow=2)
```

Next, we look at the __education_level__ variable. We see that there is an unnecessary level of detail. 

```{r, echo = FALSE, results = "hide"}
 summary(data_all$education_level)
  
  
  #Make the bar graph of frequencies  
  p2 <- ggplot(data=data_all, aes(fct_infreq(factor(education_level)))) +
    geom_bar(color = "black", fill = "lightgoldenrod1", alpha=.9) +
    theme_clean()+
    labs(x = "", y = "Count") +
    ggtitle("Educational Attainment") + 
    scale_y_continuous(labels = function(x) format(x, big.mark = ",",
                                                   scientific = FALSE))+
    scale_x_discrete(labels = function(x) str_wrap(x, width = 5))

```

```{r fig.width=10, fig.height=4, fig.align="left", echo = FALSE}
p2
```

Distinguishing between people who completed 7th-8th grade, versus 10th, versus 12th grade, and so on, would create additional degrees of freedom that are unlikely to add value to our predictive model. We therefore recode the variable so that all levels below _High School Graduate_ are combined into a single class which we call _12th and below_. 

```{r, echo = FALSE, results = "hide"}
data_all$education_level <- combineLevels(data_all$education_level, c('Preschool', '1st-4th', '5th-6th', '7th-8th', '9th', '10th', '11th', '12th'), newLabel = "12th and below")
```

```{r, echo = FALSE}
levels(data_all$education_level)
```

The resulting summary shows that high school graduates are the most common type of worker (32.7%), followed by people who completed some college (21.9%) and those who obtained a Bachelor's degree. We also see that the smallest proportion of people making more than $50K/year is found among those with educational attainment of 12th grade and below, while the highest proportion (75.4%) is found among those with professional degrees beyond a Bachelor's degree (_Prof-school_). This is to be expected.

```{r, echo=FALSE} 
 proportion3 <- data_all %>%
    group_by(education_level) %>%
    tally() %>%
    mutate(pct = n / sum(n), y_label = paste0(round(pct*100, 1), "%"))
  
  
  p3 <- proportion3 %>% 
    ggplot(aes(x = reorder(education_level, -pct), y = pct)) +
    geom_bar(stat = "identity", position = "dodge", fill = "lightgoldenrod1", color = "black", alpha=.9) +
    geom_text(aes(label = y_label), position = position_dodge(width=.9), vjust = -.25) +
    scale_y_continuous(labels = percent_format(accuracy = 1))+
    xlab("")+
    ylab("")+
    theme_clean()+
    ggtitle("Educational Attainment") 

  
  #Make stacked bars
  proportion4 <- data_all %>%
    group_by(education_level, income_category) %>%
    tally() %>%
    group_by(education_level) %>%
    mutate(pct = n / sum(n), y_label = paste0(round(pct*100, 1), "%"))
  
  
  p4 <- proportion4 %>% 
    ggplot(aes(y = pct, x = education_level, fill = income_category))+
    geom_bar(position = "fill", stat = "identity", alpha = .75) +
    scale_fill_viridis(discrete=TRUE, option = "viridis")+
    geom_text(aes(label = y_label), position = position_stack(vjust = .5), color = "black") +
    xlab("")+
    ylab("")+
    scale_y_continuous(labels = percent_format(accuracy = 1))+
    ggtitle("Breakdown of Income Class by Education Attainment") + 
    theme_clean()
  
```


```{r fig.width=10, fig.height=8, fig.align="left", echo = FALSE}
grid.arrange(p3,p4, nrow=2)
```

Next, we examine the __marital_status__ variable. We find that civilian married individuals make up the greatest proportion of the data (46.6%), and that these same people are more likely to be earning more than $50K/year (45.4%). 

```{r, echo=FALSE, results = "hide"} 
 summary(data_all$marital_status)
  
  #Make bar chart
  proportion5 <- data_all %>%
    group_by(marital_status) %>%
    tally() %>%
    mutate(pct = n / sum(n), y_label = paste0(round(pct*100, 1), "%"))
  
  
  p5 <- proportion5 %>% 
    ggplot(aes(x = reorder(marital_status, -pct), y = pct)) +
    geom_bar(stat = "identity", position = "dodge", fill = "lightgoldenrod1", color = "black", alpha=.9) +
    geom_text(aes(label = y_label), position = position_dodge(width=.9), vjust = -.25) +
    scale_y_continuous(labels = percent_format(accuracy = 1))+
    xlab("")+
    ylab("")+
    theme_clean()+
    ggtitle("Marital Status") 
  
  
  #Make stacked bars
  proportion6 <- data_all %>%
    group_by(marital_status, income_category) %>%
    tally() %>%
    group_by(marital_status) %>%
    mutate(pct = n / sum(n), y_label = paste0(round(pct*100, 1), "%"))
  
  
  p6 <- proportion6 %>% 
    ggplot(aes(y = pct, x = marital_status, fill = income_category))+
    geom_bar(position = "fill", stat = "identity", alpha = .75) +
    scale_fill_viridis(discrete=TRUE, option = "viridis")+
    geom_text(aes(label = y_label), position = position_stack(vjust = .5), color = "black") +
    xlab("")+
    ylab("")+
    ggtitle("Breakdown of Income Class by Marital Status") + 
    scale_y_continuous(labels = percent_format(accuracy = 1))+
    scale_x_discrete(labels = function(x) str_wrap(x, width = 5))+
    theme_clean()
  
```

```{r fig.width=10, fig.height=8, fig.align="left", echo = FALSE}
grid.arrange(p5,p6, nrow=2)
```

Looking at the __occupation__ variable, we note that occupations are fairly evenly distributed in the sample, and that, unsurprisingly, individuals in managerial roles (_Exec-managerial_) and those with a white-collar specialty (_Prof-specialty_) are most likely to make above $50K/year, with 47.9% and 45% of respondents in that income category respectively. 


```{r, echo=FALSE, results = "hide"} 
summary(data_all$occupation)
  
  #Make bar chart
  proportion7 <- data_all %>%
    group_by(occupation) %>%
    tally() %>%
    mutate(pct = n / sum(n), y_label = paste0(round(pct*100, 1), "%"))
  
  
  p7 <- proportion7 %>% 
    ggplot(aes(x = reorder(occupation, -pct), y = pct)) +
    geom_bar(stat = "identity", position = "dodge", fill = "lightgoldenrod1", color = "black", alpha=.9) +
    geom_text(aes(label = y_label), position = position_dodge(width=.9), vjust = -.25) +
    scale_y_continuous(labels = percent_format(accuracy = 1))+
    xlab("")+
    ylab("")+
    scale_x_discrete(labels = function(x) str_wrap(x, width = 5))+
    theme_clean()+
    ggtitle("Type of Occupation") 
  
  
  #Make stacked bars
  proportion8 <- data_all %>%
    group_by(occupation, income_category) %>%
    tally() %>%
    group_by(occupation) %>%
    mutate(pct = n / sum(n), y_label = paste0(round(pct*100, 1), "%"))
  
  
  p8 <- proportion8 %>% 
    ggplot(aes(y = pct, x = occupation, fill = income_category))+
    geom_bar(position = "fill", stat = "identity", alpha = .75) +
    scale_fill_viridis(discrete=TRUE, option = "viridis")+
    geom_text(aes(label = y_label), position = position_stack(vjust = .5), color = "black") +
    xlab("")+
    ylab("")+
    scale_y_continuous(labels = percent_format(accuracy = 1)) +
    scale_x_discrete(labels = function(x) str_wrap(x, width = 4)) +
    ggtitle("Breakdown of Income Class by Type of Occupation") + 
    theme_clean()
```

```{r fig.width=10, fig.height=4, fig.align="left", echo = FALSE}
  plot_grid(p7)
```

```{r fig.width=14, fig.height=8, fig.align="left", echo = FALSE}
  plot_grid(p8)
```

The __sex__ variable reveals that the majority of survey responders (67.5%) are male, and that men are more likely than women to earn more than $50K/year (32.1% versus 11.4%).

```{r, echo=FALSE, results = "hide"} 
summary(data_all$sex)
    
    #Make bar chart
    proportion9 <- data_all %>%
      group_by(sex) %>%
      tally() %>%
      mutate(pct = n / sum(n), y_label = paste0(round(pct*100, 1), "%"))
    
    
    p9 <- proportion9 %>% 
      ggplot(aes(x = reorder(sex, -pct), y = pct)) +
      geom_bar(stat = "identity", position = "dodge", fill = "lightgoldenrod1", color = "black", alpha=.9) +
      geom_text(aes(label = y_label), position = position_dodge(width=.9), vjust = -.25) +
      scale_y_continuous(labels = percent_format(accuracy = 1))+
      xlab("")+
      ylab("")+
      theme_clean()+
      ggtitle("Sex") 
    
    #Make stacked bars
    proportion10 <- data_all %>%
      group_by(sex, income_category) %>%
      tally() %>%
      group_by(sex) %>%
      mutate(pct = n / sum(n), y_label = paste0(round(pct*100, 1), "%"))
    
    
    p10 <- proportion10%>% 
      ggplot(aes(y = pct, x = sex, fill = income_category))+
      geom_bar(position = "fill", stat = "identity", alpha = .75) +
      scale_fill_viridis(discrete=TRUE, option = "viridis")+
      geom_text(aes(label = y_label), position = position_stack(vjust = .5), color = "black") +
      xlab("")+
      ylab("")+
      ggtitle("Breakdown of Income Class by Sex") + 
      scale_y_continuous(labels = percent_format(accuracy = 1))+
      theme_clean()
```

```{r fig.width=10, fig.height=6, fig.align="left", echo = FALSE}
grid.arrange(p9,p10, nrow=1)  
```

We skip over the __relationship__ variable because the metadata does not provide enough information on what it means. While census bureau documentation explains that the __relationship__ question denotes each household member's relationship to the primary householder (the person who owns or rents the housing unit), the variable in our dataset does not contain a class which is the equivalent of "self". The levels and their frequencies are as follows:

```{r, echo=FALSE}
summary(data_all$relationship) 
```

The absence of a class equivalent to "self" suggests that the UCI sample excludes primary householders altogether, yet the fact that most survey respondents are male (67.5%) and "husband" is the most common class of the __relationship__ variable suggests otherwise. Another possibility is that __relationship__ actually denotes the role of the individual in the household. This would explain the high prevalence of males and "husband" as the relationship type; however we do not have enough information to ascertain our hypothesis. 

We move on to examine variability along the __race__ variable, and note that an overwhelming majority (86%) of the survey respondents are white, and that the rate of people earning more than $50K/year is highest among the Asian/Pacific Islander ethnic group (28.3%), followed by caucassians at 26.2%.  

```{r, echo=FALSE, results = "hide"} 
summary(data_all$race)
  
  #Make bar chart
  proportion11 <- data_all %>%
    group_by(race) %>%
    tally() %>%
    mutate(pct = n / sum(n), y_label = paste0(round(pct*100, 1), "%"))
  
  
  p11 <- proportion11 %>% 
    ggplot(aes(x = reorder(race, -pct), y = pct)) +
    geom_bar(stat = "identity", position = "dodge", fill = "lightgoldenrod1", color = "black", alpha=.9) +
    geom_text(aes(label = y_label), position = position_dodge(width=.9), vjust = -.25) +
    scale_y_continuous(labels = percent_format(accuracy = 1))+
    xlab("")+
    ylab("")+
    theme_clean()+
    scale_x_discrete(labels = function(x) str_wrap(x, width = 5))+
    ggtitle("Race/Ethnic Origin") 
  
  
  #Make stacked bars
  proportion12 <- data_all %>%
    group_by(race, income_category) %>%
    tally() %>%
    group_by(race) %>%
    mutate(pct = n / sum(n), y_label = paste0(round(pct*100, 1), "%"))
  
  
  p12 <- proportion12 %>% 
    ggplot(aes(y = pct, x = race, fill = income_category))+
    geom_bar(position = "fill", stat = "identity", alpha = .75) +
    scale_fill_viridis(discrete=TRUE, option = "viridis")+
    geom_text(aes(label = y_label), position = position_stack(vjust = .5), color = "black") +
    xlab("")+
    ylab("")+
    ggtitle("Breakdown of Income Class by Race") + 
    scale_y_continuous(labels = percent_format(accuracy = 1))+
    scale_x_discrete(labels = function(x) str_wrap(x, width = 5))+
    theme_clean()
```

```{r fig.width=10, fig.height=6, fig.align="left", echo = FALSE}
grid.arrange(p11,p12, nrow=1)  
```

Finally, we examine the structure of the __native_country__ variable. A summary of this categorical feature shows that it contains 41 different levels, most of which have very few observations. 

```{r, echo=FALSE}
levels(data_all$native_country)
```

Because keeping so many levels as an input is likely to add unnecessary degrees of freedom and bring down our model's performance, we collapse countries of origin into regional categories. We assign these categories to a new variable, __native_region__:

```{r}

Asia <- c("Cambodia", "India", "Laos", "Thailand", 
          "Vietnam", "Hong", "Iran", "China",
          "Japan", "Philippines", "Taiwan")
      
Europe <- c("France", "Italy", "Poland", "Scotland", 
            "Germany", "Portugal", "Yugoslavia", "England", 
            "Greece","Holand-Netherlands", "Hungary", "Ireland")
    
North_America <- c("Outlying-US(Guam-USVI-etc)", 
                   "Canada", "United-States", "Puerto-Rico")
    
Latin_America_Carrib <- c("Columbia", "Ecuador", "Guatemala", 
                          "Honduras", "Cuba", "El-Salvador", "Haiti", 
                          "Jamaica", "Mexico", "Peru", "Trinadad&Tobago", 
                          "Dominican-Republic", "Nicaragua")
    
Unknown <- c("South")
    
data_all <- data_all %>% 
  mutate(native_region = case_when(native_country %in% Asia ~ "Asia", 
                                   native_country %in% Europe ~ "Europe", 
                                   native_country %in% North_America ~"North_America",
                                   native_country %in% Latin_America_Carrib 
                                   ~ "Latin_America_Carrib", native_country %in% 
                                     Unknown ~ "Unknown"))

```

Our __native_region__ feature will replace the __native_country__ variable. It has the following values:

```{r, echo = FALSE}

data_all$native_region <- as.factor(data_all$native_region)
    
levels(data_all$native_region)

```
Here, "Unknown" corresponds to the entries with the ambiguous value of "South".

#### __II.2 Continuous Attributes__

We move on to examine the distribution of values along our continuous predictors. We see that 50% of survey respondents are between the ages of 28 and 47, and and that the age variable has a right-skewed distribution, as there is a higher concentration of individuals below the median age than above it. This certainly helps to explain the low prevalence of individuals earning at least $50K/year, seeing how this value is very close to the median income in the U.S ($52,942) in 1994. We also see from looking at boxplots and density plots of age along our two income categories that the age distribution of people earning more than $50K/year is older and and has less variability than that of people earning below that threshold. 

```{r, echo = FALSE}
  
  summary(data_all$age)
  
  #Make the histogram
  p13 <- data_all %>% ggplot(aes(age)) +
    geom_histogram(binwidth = 1, color = "black", fill = "purple4", alpha=.95) +
    theme_clean()+
    labs(x = "", y = "Count") +
    ggtitle("Age Distribution") 
  
  
  #Make box plots
  p14 <- data_all %>% ggplot(aes(income_category, age, fill = income_category)) +
    geom_boxplot(alpha = .8) +
    xlab("")+
    ylab("Age")+
    ggtitle("Age by Income Category")+
    scale_fill_viridis(discrete=TRUE, option = "viridis")+
    theme_clean() 
    
  #Make density plot
  p15 <- data_all %>% ggplot(aes(x = age, fill = income_category)) +
    geom_density(position = "identity", alpha = .6) +
    ggtitle("Age Density by Income Category")+
    scale_fill_viridis(discrete=TRUE, option = "viridis")+
    xlab("")+
    ylab("Density")+
    theme_clean()
```

```{r fig.width=10, fig.height=4, fig.align="left", echo = FALSE}
grid.arrange(p13,p14,nrow=1)  
```

```{r fig.width=10, fig.height=4, fig.align="left", echo = FALSE}
grid.arrange(p15)  
```

Finally, we examine the distribution of values contained in the __capital_loss__ and the __capital_gain__ variables. Density plots of these attributes suggest that the values are heavily skewed towards zero.

```{r, echo = FALSE, results = "hide"}
  summary(data_all$capital_loss)
  summary(data_all$capital_gain)
  
  p16 <- data_all %>% ggplot(aes(x = capital_loss)) +
    geom_density(fill = "purple4", alpha=.95) +
    theme_clean()+
    labs(x = "", y = "Density") +
    ggtitle("Distribution of Capital Loss") 
  
  
  p17 <- data_all %>% ggplot(aes(x = capital_gain)) +
    geom_density(fill = "gold", alpha=.95) +
    theme_clean()+
    labs(x = "", y = "Density") +
    ggtitle("Distribution of Capital Gain") 
```  
 
```{r fig.width=10, fig.height=4, fig.align="left", echo = FALSE}
grid.arrange(p16,p17,nrow=1)  
```

We perform a summary computation to ascertain this and see that there are indeed very few instances of values that differ from zero. 91.6% of the survey respondents have no capital gain, and and 95.3% have no capital loss. 

```{r, echo = FALSE,results="hide"}
data_all %>% filter(capital_gain == 0) %>% summarise(value = 0, prevalence = n()/45222*100)
    data_all %>% filter(capital_loss == 0) %>% summarise(value = 0, prevalence = n()/45222*100)
```  

Such a high prevalence of zeros in a continuous variable is likely to distort our predictions. We must therefore create bins so that we can treat both __capital_gain__ and __capital_loss__ as categorical features in our predictive model. In order to compute these bins, we first explore the distribution of the non-zero values for each variable. We compute the quantiles and get the below results:


```{r, echo = FALSE}
q_gain <- quantile(x = subset(data_all$capital_gain, data_all$capital_gain != 0), probs = seq(0, 1, .25))
q_loss <- quantile(x = subset(data_all$capital_loss, data_all$capital_loss != 0), probs = seq(0, 1, .25))
    
quantiles <- data.frame(Capital_Gain = q_gain, Capital_Loss = q_loss)

```

```{r, echo=FALSE}
quantiles %>% kable() %>%
  kable_styling(latex_options = c("striped", "hold_position"), full_width = T)
```  

We note that the interquartile range for __capital_gain__ (10,620) is significantly larger than the IQR for __capital_loss__ (305). Then, we create histograms and boxplots of the non-zeros values of __capital_gain__ and __capital_loss__ to further investigate the distribution of these attributes before creating cutoff values for our bins. 


```{r, echo = FALSE}

 pos_gain <- subset(data_all, capital_gain != 0)
 neg_loss <- subset(data_all, capital_loss != 0)
    
p18 <- pos_gain %>% ggplot(aes(capital_gain)) +
      geom_histogram(binwidth = 2500, color = "black", fill = "gold", alpha=.95) +
      theme_clean()+
      labs(x = "", y = "Count") +
      scale_x_continuous(labels = comma) +
      scale_y_continuous(breaks = seq(0,1000,50)) +
      ggtitle("Histogram of Non-Zero Capital Gain") 
    
    p19 <- neg_loss %>% ggplot(aes(capital_loss)) +
      geom_histogram(binwidth = 200, color = "black", fill = "purple4", alpha=.95) +
      theme_clean()+
      labs(x = "", y = "Count") +
      scale_x_continuous(labels = comma) +
      scale_y_continuous(breaks = seq(0,1000,50)) +
      ggtitle("Histogram of Non-Zero Capital Loss") 
```

```{r fig.width=10, fig.height=4, fig.align="center", echo = FALSE}
grid.arrange(p18,p19,nrow=1)  
```

```{r, echo = FALSE}
p20 <- pos_gain %>% ggplot(aes(x = factor(0), y = capital_gain)) +
     geom_boxplot(alpha = .8, fill = "gold") +
     xlab("")+
     ylab("Capital Gain")+
     ggtitle("Boxplot of Non-Zero Capital Gain") +
     scale_y_continuous(breaks = seq(0, 100000, 5000)) +
     stat_summary(fun.y = mean, 
                  geom = 'point', 
                  shape = 19,
                  color = "red",
                  cex = 2) +
     scale_fill_viridis()+
     theme_clean()  
   
   p21 <- neg_loss %>% ggplot(aes(x = factor(0), y = capital_loss)) +
     geom_boxplot(alpha = .8, fill = "darkorchid4") +
     xlab("")+
     ylab("Capital Loss") +
     scale_y_continuous(breaks = seq(0, 5000, 500)) +
     stat_summary(fun.y = mean, 
                  geom = 'point', 
                  shape = 19,
                  color = "red",
                  cex = 2) + 
     ggtitle("Boxplot of Non-Zero Capital Loss") +
     theme_clean()  
```

```{r fig.width=10, fig.height=4, fig.align="center", echo = FALSE}
grid.arrange(p20,p21,nrow=1)  
```

We see that the distribution of non-zero values of __capital_gain__ is right-skewed, with most pepple having a capital gain between $2,500 and $27,500 - the most common value being $7,500 - and that there is a handful of outliers with a capital gain of $100,000. The distribution for __capital_loss__ on the other hand, is more symmetric, with most values falling between $1,400 and $2,400 - the most common value being $2,000 - and a large number of outliers above and below this window.

We then create the __cap_gain__ variable which has the following levels for the corresponding ranges of __capital_gain__:

* Values equal to or below the first quartile of non-zero values ($3,464): Low
  
* Values between the first and third quartile of non-zero values ($3,464 to $14,084): Medium
  
* Values above the third quartile of non-zero values ($14,084): High

We do the same for __capital_loss__ and create the __cap_loss__ variable which has following levels:

* Values equal to or below the first quartile of non-zero values ($1,672): Low
  
* Values between the first and third quartile of non-zero values ($1,672 to $1,977): Medium

* Values above the third quartile of non-zero values ($1,977): High

The code looks like this:

```{r, results = "hide", warning = FALSE}
data_all <- data_all %>% 
  mutate(cap_gain = ifelse(capital_gain <= 3464, "Low",
                               ifelse(capital_gain > 3464 & capital_gain < 14084, 
                                      "Medium", "High")))
    
    
data_all$cap_gain <- factor(data_all$cap_gain,
                                ordered = TRUE,
                                levels = c("Low", "Medium", "High"))
    
data_all <- data_all %>% 
  mutate(cap_loss = ifelse(capital_loss <= 1672, "Low",
                               ifelse(capital_gain > 1672 & capital_gain < 1977, 
                                      "Medium", "High")))
    
data_all$cap_loss <- factor(data_all$cap_loss,
                                ordered = TRUE,
                                levels = c("Low", "Medium", "High"))
    
```


After making these modifications to our features, we proceed to conduct a correlation analysis in order to assess how interrelated they are. This is important because in order to build an efficient predictive model, it is best to only include variables that __uniquely__ explain some amount of variance in the outcome. Because correlations are most easily computed on numeric variables, we convert all the values of our categorical variables to numeric levels after subsetting our data to exclude the outcome variable (__income_category__), the demographic __weight__ variable, and the variables we no longer wish to retain after making our modifications: the continous __capital_gain__ and __capital_loss__ features, and the __native_country__ feature. The correlation matrix does not suggest any strong correlation between any of our features, so we can retain of all the selected features in training a model to predict the income brackets. 

```{r, echo = FALSE}
#Remove undesired features and outcome variable from the dataset
  cor_vars <- subset(data_all, select = -c(final_weight,
                                           income_category,
                                           native_country,
                                           capital_gain,
                                           capital_loss))
  
  #Convert all factor levels to numeric
  cor_vars$workclass <- as.numeric(cor_vars$workclass)
  cor_vars$education_level <- as.numeric(cor_vars$education_level)
  cor_vars$marital_status <- as.numeric(cor_vars$marital_status)
  cor_vars$occupation <- as.numeric(cor_vars$occupation)
  cor_vars$relationship <- as.numeric(cor_vars$relationship)
  cor_vars$race <- as.numeric(cor_vars$race)
  cor_vars$sex <- as.numeric(cor_vars$sex)
  cor_vars$native_region <- as.numeric(cor_vars$native_region)
  cor_vars$cap_gain <- as.numeric(cor_vars$cap_gain)
  cor_vars$cap_loss<- as.numeric(cor_vars$cap_loss)
  
  #Compute the correlations and create the correlation matrix, then heatmap
  cordata <- round(cor(cor_vars),2)
  melted_cordata <- melt(cordata)
  
  p22 <- melted_cordata %>% ggplot(aes(x=X1, y=X2, fill=value, label=value)) +
    geom_tile() +
    scale_fill_viridis() +
    xlab("") +
    ylab("") + 
    geom_text(color = "white") +
    ggtitle("Correlation Matrix of Selected Features")
  
```

```{r fig.width=12, fig.height=10, fig.align="left", echo = FALSE}
grid.arrange(p22)  
```

## __C.Paritionning and Training__

We begin by subsetting our dataset to retain on the following features: __age__, __workclass__, __education_level__, __marital_status__, __occupation__, __relationship__, __race__, __sex__, __hours_per_week__, __native_region__, __cap__gain__, and __cap_loss__. Finally, we convert the __cap__gain__ and __cap_loss__ features to numeric for both models because it is an ordinal feature. The results are below:

We inspect the dataframe structure on last time to ensure that our categorical variables as stored as factors.

```{r, echo = FALSE}
 #Retain only selected features from the dataset and partiion the data 
  
  data_final <- subset(data_all, select = -c(final_weight,
                                             native_country,
                                             capital_gain,
                                             capital_loss))
  data_final$cap_gain <- as.numeric(data_final$cap_gain)
  data_final$cap_loss <- as.numeric(data_final$cap_loss)
  
  data_final$cap_gain <- as.numeric(data_final$cap_gain)
  data_final$cap_loss <- as.numeric(data_final$cap_loss)
  

  str(data_final)
```

We then partition the data into training and testing sets to which we allocate 90% and 10% of the data respectively. The resulting training set has 40,699 observation and the testing set has 4,523 observation. We make sure to set a seed so that each time the program is run the partition remains the same. Our selected classification models in order of preference are: support vector machines, random forest, gradient boosting, and logistic regression. As mentionned in the introduction, we will be using balanced accuracy (F1 Score, computed with the F_meas function of the caret package) to optimize the models and assess their performance.  

```{r, echo = FALSE}
 #Retain only selected features from the dataset and partiion the data 
  
  data_final <- subset(data_all, select = -c(final_weight,
                                             native_country,
                                             capital_gain,
                                             capital_loss))
  str(data_final)
  
  set.seed(1) 
  test_index <- createDataPartition(y = data_final$income_category, times = 1, p = 0.1, list = FALSE)
  training <- data_final[-test_index,]
  testing <- data_final[test_index,]

```

We begin by training a logistic regression model because it is one of the most simple and commonly used unsupervised machine learning algorithms for classification tasks, is easy to implement, and is often used as a baseline for binary classification. The model performs binary classification by estimating a linear relationship between the features and outcome variable, and converting its predictions to probabilities between zero and 1 using the sigmoid function. These probabilities are then be mapped to one of the two outcome classes using a pre-determined probability threshold of 0.5. While we would have liked to change the threshold value due to the unbalanced nature of our outcome (high prevalence of people earning <=50K), the caret package does not allow for parameter tuning. 

We choose a Beta value of 0.9 for our balaanced accuracy (F1 score) computation using the F_meas function. Beta is used to weight precision and recall and a value below 1 signifies that we wish to assign more weight to precision over recall. This beta will be fixed for all of our models in order to allow for comparability.

We train the logistic model on our training set using all of our selected features, using the controlTrain() function to specify a reseampling scheme of 20-fold cross validation. This generates 20 non-overlapping, independent random samples from our training set, which are used to generate a cross-validated F1 score once the predictions are applied to our testing set.

We train two logistic models: one with the categorical features as they are, and one with one hot encoding of our nominal categorical features, because we are curious about the claim that logistic regression performs better with with continuous, binary, and ordinal categorical features. By one hot encoding we mean creating dunmmy variables for each level of our nominal features such that each of these variables has only two possible values: zero and one. For instance, __marital_status__ is be split into the following variables: "Divorced"              "Married-AF-spouse"     "Married-civ-spouse"    "Married-spouse-absent"
"Never-married"         "Separated"             "Widowed", each of which is binary.    

We proceed to one-hot encode the rest of the nominal features for the second iteration of our logistic classifier. These are: __workclass__, __education_level__, __marital_status__, __occupation__, __relationship__, __race__, and __native_region__. The code looks like this:

```{r, results = "hide", warning = FALSE}

  ohe_features = c("workclass", "education_level", "marital_status", "occupation", 
                   "relationship","race", "native_region")
  
  #Training Set
  dummies_train <- dummyVars(~ workclass + education_level + marital_status +
                               occupation + relationship + race + native_region,
                             data = training)
  
  
  ohe_dummies_train <- as.data.frame(predict(dummies_train, newdata = training))
  training_ohe <- cbind(training[,-c(which(colnames(training) %in% ohe_features))],
                        ohe_dummies_train)
  
  training_ohe$cap_gain <- as.numeric(training_ohe$cap_gain)
  training_ohe$cap_loss <- as.numeric(training_ohe$cap_loss)
  
  
  #Testing Set
  dummies_test <- dummyVars(~ workclass + education_level + marital_status +
                              occupation + relationship + race + native_region,
                            data = testing)
  
  ohe_dummies_test <- as.data.frame(predict(dummies_test, newdata = testing))
  testing_ohe <- cbind(testing[,-c(which(colnames(testing) %in% ohe_features))],
                       ohe_dummies_test)
  
  testing_ohe$cap_gain <- as.numeric(testing_ohe$cap_gain)
  testing_ohe$cap_loss <- as.numeric(testing_ohe$cap_loss)
  
  #TRAINING ATTEMPT 1: WITHOUT ONE HOT ENCODED VARIABLES
  # Define cross validation parameters
  train_control <- trainControl(method = "cv", number = 20, p = .8)
  
  # Train the model on training set
  logit_fit1 <- train(income_category ~ .,
                 data = training,
                 trControl = train_control,
                 method = "glm",
                 family=binomial())
  
  #Fit the model on testing set
  predictions1 <- predict(logit_fit1, testing)
  
  #Compute balanced accuracy
  BA_1 <- F_meas(data = predictions1, reference = testing$income_category, beta = .9)
  
  #TRAINING ATTEMPT 2: WITH ONE HOT ENCODED VARIABLES
  
  logit_fit2 <- train(income_category ~ .,
                     data = training_ohe,
                     trControl = train_control,
                     method = "glm",
                     family=binomial())
  
  #Fit the model on testing set
  predictions2 <- predict(logit_fit2, testing_ohe)
  
  #Compute balanced accuracy
  BA_2 <- F_meas(data = predictions2, reference = testing_ohe$income_category, beta = .9)
  
  results <- data.frame(Method = c("Simple Logistic Regression", 
                                   "Logistic Regression With One-Hot Encoding"), 
                        Balanced_Accuracy = c(BA_1, BA_2))
```

After training each model we generate predictions for the testing set and obtain the below results:

```{r, echo=FALSE}
results %>% kable() %>%
  kable_styling(latex_options = c("striped", "hold_position"), full_width = T)
```  

As the results table shows, one hot encoding on produces a very negligible improvement in our logistic model's performance.

The second model we train is the Support Vector Machines (SVMs) algorithm. This supervised, non-probabilistic classifier has become very popular for producing very high accuracy while using a minimal amount of computational power. It works by partioning the feature space into two or more groups with decision boundaries (which take the shape of n-dimensional hyperplanes, n denoting the number of features used) that separate the data into our outcome classes. The algorithm iterates through several decision bounderies and selects the one that maximizes the margin between the points in each class, using a regularization parameter that strikes a balance between expected loss and margin maximization. What we call support vectors is the collection of points that lie closest to the hyperplane and therefore influence its position and orientation. Support vector machines are especcially effective for highly dimensional data (i.e. data with many features) because they make use of kernels to enlarge the feature space and accommodate a non-linear boundary between the outcome classes. We use the _svm_ function from the e107 package to train this model.

We scale the values of our continuous features before implementing the algorithm so that each feature's contribution to the distance calculations is appromixately proportionate. We then train the model using the same cross validation parameters specified for the logistic regression. The code looks like this:


```{r, results = "hide", warning = FALSE,message = FALSE}
#2) SUPPORT VECTOR MACHINES#
  ############################
  
  #Scale the numeric features in both the training and the teting set 
  
  training_svm <- training 
  training_svm$age <- scale(training_svm$age)
  training_svm$hours_per_week <- scale(training_svm$hours_per_week)
 
  testing_svm <- testing 
  testing_svm$age <- scale(testing_svm$age)
  testing_svm$hours_per_week <- scale(testing_svm$hours_per_week)
  
  str(training_svm)
  str(testing_svm)
  
  #Train the model using same cross validation parameters as before
  train_control <- trainControl(method = "cv", number = 20, p = .8)
  
  library(kernlab)
  
  #grid <- expand.grid(C = seq(0,2,.25))
  
  svm_linear <- train(income_category ~ .,
                      data = training_svm,
                      trControl = train_control,
                      method = "svmLinear",
                      tuneLength = 10)

  warnings()
  
  #Generate predictions
  predictions3 = predict(svm_linear, testing_svm) 
  
  #Compute balanced accuracy of linear support vector machine
  BA_3 <- F_meas(data = predictions3, reference = testing_svm$income_category, beta = .9)
  
  #Add results to a table
  results <- bind_rows(results, 
                       data.frame(Method = "Support Vector Machines with Linear Kernel", 
                                  Balanced_Accuracy = BA_3))

```

Our results are dissapointing, given the amount of time and commputational power used by the SVM model.

```{r, echo=FALSE}
results %>% kable() %>%
  kable_styling(latex_options = c("striped", "hold_position"), full_width = T)
```  



The final model we fit is a tree-learning classifier using the Extreme Gradient Boosting package. Used for both regression and classification problems, Extreme Gradient Boosting produces a prediction model in the form of an ensemble of weak prediction models, and allows for optimization through an arbitrary differentiable loss function. Specifically, it considers the potential loss for all possible splits to create a new branch by looking at the distribution of features across all data points in a branch and using this information to reduce the search space of possible feature splits.

What makes it so appealing - in our case, much more appealing than the random forest algorithm - is its  ability to perform parallel computations on a single machine with speed, while using very little compuational power.  Moroever, the xgboost is known to perform well when the training data contains a mix of categorical an numeric features, as is the case with our dataser. The algorithm possesse a large numbe of hyperparameters desinged to prevent overfitting.

Using cross validation (bootstrap aggregation being the default setting), this non-probabilistic classifier iterates through a collection of decision trees using binary recursive paritionning of the predictor space. Feature selection is random for each tree, and the algortihm selects the combination of paritions that minmize the residual sum of squares for continuous outcomes or the Gini index for categorical outcomes. It then averages the conditional expectation of the decision trees to obtain a final prediction for the class of each observation. Bootstrap aggregation refers to the cross validation method of treating the training set as a population and taking several samples of the same size with replacement. 

There are a few preparatory steps to undertake before training. First, we use the one-hot encoded versions of our training and testing sets because the XGBoost model only accepts numeric inputs. Then, we separate the outcome varibale from the feature data, convert the feature sets into matrices, and reattach the labels to the feature sets in a xgb.DMatrix object, which is specific to the XGBoost. The code looks like this:

```{r, results = "hide", warning = FALSE}
  #Convert sex feature (still coded as a factor) to numeric
    training_ohe$sex <- as.numeric(training_ohe$sex)
    testing_ohe$sex <- as.numeric(testing_ohe$sex)

  #Separate target variable (income_category) from training and testing sets 
  # (one hot encoded) and convert to numeric
  
  tr_labels <- training_ohe$income_category
  tr_labels <- as.numeric(tr_labels) -1 
  str(tr_labels)
  
  ts_labels <- testing_ohe$income_category
  ts_labels <- as.numeric(ts_labels) -1 
  str(ts_labels)
  
  
  training_ohe <- training_ohe %>% select(-income_category)
  testing_ohe <- testing_ohe %>% select(-income_category)
  
  
  #Convert training and testing sets into matrices then reattach to labels
  xgtrain <- matrix(as.numeric(unlist(training_ohe)),nrow=nrow(training_ohe))
  xgtest <- matrix(as.numeric(unlist(testing_ohe)),nrow=nrow(testing_ohe))
  
  str(xgtrain)
  length(tr_labels)
  
  str(xgtest)
  length(ts_labels)
  
  xgtrain <- xgb.DMatrix(data = xgtrain, label = tr_labels) 
  xgtest <- xgb.DMatrix(data = xgtest, label = ts_labels)
```  

Finally, we specify our tuning parameters. XGBoost has a long list of tuning parameters meant to optimize performance and controlfor overfitting. We select only only the few we deem necessary for the purpose of this simple exercize. 

* objective = estimation function, default being linear regression; in our case we select logistic regression for binary classification, which returns predicted probabilities 
* nrounds = number of learning iterations (number of trees in the case of classification)
* max.depth = the maximum number of partitions in a tree; the more splits, the information is captured
* eta = the learning rate, i.e. degree to which the weight of each consecutive tree is reduced
* min_child_rate = minimum Hessian weight required for a new partition; when min is reached, partitions stop 
* gamma  = the minimum loss reduction required to make an additional partition
* colsample_bytree = the proportion of features supplied to a tree

We store these in a list:

```{r, message = FALSE}
  params <- list(booster = "gbtree",
                 objective = "binary:logistic",
                 max_depth = seq(2,15, 1),
                 eta = seq(0.05, 0.3, .05),
                 min_child_weight = c(1, 3, 5, 7),
                 gamma = c(0, 0.4, 0.1),
                 colsample_bytree = seq(0.3,0.8,0.1))
```  

Next, we use XGBoost's built in cross-validation module which allows us to quickly determine the ideal number of iterations (_nrounds_) without performing a grid search. We specify a maximum of 250 trees, so the module computes a cross validation error (which is an estimate for the test error) for every nround up to 250. We choose a resampling scheme of 10-fold cross validation to make the implemetation less computationally costly. 

```{r, message = FALSE, results = "hide"}
#Find the best number of iterations (trees) using cross validation module
  set.seed(0) 
  xgboost_ideal <- xgb.cv(data = xgtrain, 
                       nrounds = 250,
                       nfold = 10,
                       params = params, 
                       verbose = TRUE,
                       maximize = F,
                       stratified = T, 
                       print_every_n = 10,
                       early_stopping_rounds = 20,
                       eval_metric="error")
```

Here, _stratified_ indicates whether the sampling of cross validation folds should be stratified by the value of the outcome label; _print_every_n_ specifies the intervals for the printing of results, and _early_stopping_rounds_ terminates training when performance - measured by the testing error - does not improve after the specifed number of rounds.  

```{r}
#Look at the results (note: CV test error is an optimistic estimate of the actual test error)
  xgboost_ideal
```

The results indicate that the ideal number of training iterations is 240, so we train the model using an _nrounds_ of 240. 

```{r, message = FALSE, results = "hide"}
#Train model using ideal number of iterations found above
  
  xgboost_fit <- xgb.train(data = xgtrain, 
                          nrounds = 240,
                          nfold = 10,
                          params = params, 
                          verbose = TRUE,
                          maximize = F,
                          print_every_n = 10,
                          early_stopping_rounds = 20,
                          watchlist = list(val=xgtest,train=xgtrain),
                          eval_metric="error")
```

Finally, we generate the predictions for the testing set and convert the resulting probabilites into an outcome class, and compute the balanced accuracy which we add to our results table.The code looks like this:

```{r, results = "hide", warning = FALSE}
 #Generate predictions, and convert to binary by using threshold of .5 
  predictions4 = predict(xgboost_fit, xgtest) 
  predictions4 <- ifelse (predictions4 > 0.5,1,0)

  #Compute balanced accuracy of extreme gradient boosting
  ts_labels <- as.factor(ts_labels)
  predictions4 <- as.factor(predictions4)
  BA_4 <- F_meas(data = predictions4, reference = ts_labels, beta = .9)
  
  #Add results to a table
  results <- bind_rows(results, 
                       data.frame(Method = "Gradient Boosted Tree", 
                                  Balanced_Accuracy =  BA_4))
```

Looking at at the results table, we see that the balanced accurary obtained with the XBBoost alogirthm is slightly lower than that achieved with a simple logistic regression model. This is a very good illustration of how discriminative models generally outperform generative models in classification tasks. 

```{r, echo=FALSE, message=FALSE, error=FALSE, warning=FALSE}
results %>% kable() %>%
  kable_styling(latex_options = c("striped", "hold_position"), full_width = T)
```  

